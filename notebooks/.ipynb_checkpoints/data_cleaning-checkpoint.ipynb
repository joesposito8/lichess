{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'..\\data\\data.csv')\n",
    "data = raw_data[raw_data['winner'] != .5]\n",
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that Clock Times and Evals Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data.at[i, 'clocks'] = eval(data.at[i, 'clocks'])\n",
    "    data.at[i, 'evals'] = eval(data.at[i, 'evals'])\n",
    "    \n",
    "data = data[data['clocks'].str.len() == data['evals'].str.len()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Rating Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    data.at[index, 'white_' + row['perf'] + '_rating'] = row['white_rating']\n",
    "    data.at[index, 'black_' + row['perf'] + '_rating'] = row['black_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_test_train(df):\n",
    "    train, test = train_test_split(df, test_size=.2)\n",
    "\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_data, test_data = create_test_train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand on Evals/Clocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalClockExpander(TransformerMixin):\n",
    "    def __init__(self, num_rows_per_game=1):\n",
    "        self.num_rows_per_game = num_rows_per_game\n",
    "\n",
    "    def transform(self, df):\n",
    "        X = df.copy()\n",
    "\n",
    "        for i in range(self.num_rows_per_game-1):\n",
    "            X = pd.concat((X, df.copy()), axis=0)\n",
    "            \n",
    "        X = X.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "        X['white_clock'] = 0\n",
    "        X['black_clock'] = 0\n",
    "        X['eval'] = ''\n",
    "        X['is_mate'] = False\n",
    "\n",
    "        for i, row in X.iterrows():\n",
    "            index = random.randint(0, len(row['clocks']))\n",
    "            \n",
    "            if index == 0:\n",
    "                X.at[i, 'white_clock'] = row['clocks'][0]\n",
    "                X.at[i, 'black_clock'] = row['clocks'][1 if 1 < len(row['clocks']) else 0]\n",
    "                if '#' in row['evals'][0]:\n",
    "                    X.at[i, 'eval'] = row['evals'][0].replace(\"#\", \"\")\n",
    "                    X.at[i, 'is_mate'] = True\n",
    "                else:\n",
    "                    X.at[i, 'eval'] = row['evals'][0]\n",
    "            elif index == len(row['clocks']):\n",
    "                X.at[i, 'white_clock'] = row['clocks'][-1 if len(row['clocks'])%2==0 else -2]\n",
    "                X.at[i, 'black_clock'] = row['clocks'][-1 if len(row['clocks'])%2==1 else -2]\n",
    "                if '#' in row['evals'][-1]:\n",
    "                    X.at[i, 'eval'] = row['evals'][-1].replace(\"#\", \"\")\n",
    "                    X.at[i, 'is_mate'] = True\n",
    "                else:\n",
    "                    X.at[i, 'eval'] = row['evals'][-1]\n",
    "            else:\n",
    "                X.at[i, 'white_clock'] = row['clocks'][index if len(row['clocks'])%2==0 else index-1]\n",
    "                X.at[i, 'black_clock'] = row['clocks'][index if len(row['clocks'])%2==1 else index-1]\n",
    "                X.at[i, 'eval'] = row['evals'][index]\n",
    "                if '#' in row['evals'][index]:\n",
    "                    X.at[i, 'eval'] = row['evals'][index].replace(\"#\", \"\")\n",
    "                    X.at[i, 'is_mate'] = True\n",
    "                else:\n",
    "                    X.at[i, 'eval'] = row['evals'][index]\n",
    "\n",
    "        X.drop(columns=['clocks', 'evals'], inplace=True)\n",
    "        \n",
    "        X.reset_index(drop=True, inplace=True)    \n",
    "            \n",
    "        return X\n",
    "\n",
    "    def fit(selX, *_):\n",
    "        return self\n",
    "\n",
    "ece = EvalClockExpander(num_rows_per_game=10)\n",
    "train_data = ece.transform(train_data)\n",
    "test_data = ece.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_baseline_raw = pd.read_csv(r'..\\data\\baseline-june.csv')\n",
    "\n",
    "def columns_group(df, game_speed, round_num, number):\n",
    "    ret = [game_speed, number]\n",
    "    rows = df.loc[(round_to(df['rating_difference'], round_num) == number) & (df[\"game_speed\"] == game_speed)]\n",
    "    ret += [rows['total_points'].sum(), rows['total_games'].sum(), rows['total_draws'].sum()]\n",
    "    return ret\n",
    "\n",
    "def round_to(x, to):\n",
    "    return to * round(x/to)\n",
    "\n",
    "prob_baseline = pd.DataFrame(columns=prob_baseline_raw.columns)\n",
    "\n",
    "for gs in prob_baseline_raw['game_speed'].unique():\n",
    "    maximum = round_to(max(prob_baseline_raw[prob_baseline_raw['game_speed'] == gs]['rating_difference']), 100)\n",
    "    minimum = round_to(min(prob_baseline_raw[prob_baseline_raw['game_speed'] == gs]['rating_difference']), 100)\n",
    "    for num in range(minimum, maximum, 10):\n",
    "        prob_baseline.loc[len(prob_baseline)] = columns_group(prob_baseline_raw, gs, 10, num)\n",
    "        \n",
    "prob_baseline = prob_baseline[prob_baseline['total_games'] != 0]\n",
    "prob_baseline['winning_percentage'] = prob_baseline['total_points']/prob_baseline['total_games']\n",
    "prob_baseline = prob_baseline.drop(columns=['total_points', 'total_games', 'total_draws'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['white_username', 'black_username'], inplace=True)\n",
    "test_data.drop(columns=['white_username', 'black_username'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(r'..\\data\\data_train.csv', index=False)\n",
    "test_data.to_csv(r'..\\data\\data_test.csv', index=False)\n",
    "prob_baseline.to_csv(r'..\\data\\baseline.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
