{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'..\\data\\data_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDummifier(TransformerMixin):\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.get_dummies(X, columns=self.cols)\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingImputer(TransformerMixin):\n",
    "    def __init__(self, cols=None, strategy='0'):\n",
    "        self.cols = cols\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.cols] = X[self.cols].replace(to_replace = 0, value = np.nan)\n",
    "        if self.strategy == '0':\n",
    "            X[self.cols] = X[self.cols].fillna(value = 0)\n",
    "        if self.strategy == 'row_mean':\n",
    "            X[self.cols] = X[self.cols].fillna(X[self.cols].mean())\n",
    "            \n",
    "        return X\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDifferences(TransformerMixin):\n",
    "    def __init__(self, white_cols=None, black_cols=None):\n",
    "        self.white_cols = white_cols\n",
    "        self.black_cols = black_cols\n",
    "\n",
    "    def transform(self, X):\n",
    "        for white, black in zip(self.white_cols, self.black_cols):\n",
    "            X[white[6:] + \"_difference\"] = X[white] - X[black]\n",
    "        \n",
    "        X.drop(columns=self.white_cols, inplace=True)\n",
    "        X.drop(columns=self.black_cols, inplace=True)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modeling of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = pd.read_csv(r'..\\data\\baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to(x, to):\n",
    "    return to * round(x/to)\n",
    "\n",
    "def create_labels_data(df, label_column):\n",
    "    data = df[[column for column in df if column != label_column]]    \n",
    "    labels = df[label_column]\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def cross_val_accuracy(estimator, X, y, num_cuts=5):\n",
    "    chunks = np.array_split(list(X['id'].unique()), num_cuts)\n",
    "    scores = []\n",
    "    rmse = []\n",
    "    \n",
    "    for i, ids in enumerate(chunks):\n",
    "        fit_index = list(X[~X['id'].isin(ids)].index)\n",
    "        score_index = list(X[X['id'].isin(ids)].index)\n",
    "        \n",
    "        estimator.fit(X.drop(columns=['id']).iloc[fit_index], y.iloc[fit_index])\n",
    "        scores.append(estimator.score(X.drop(columns=['id']).iloc[score_index], y.iloc[score_index]))\n",
    "    \n",
    "    return scores, mean(scores)\n",
    "\n",
    "def cross_val_rmse(estimator, X, y, num_cuts=5):\n",
    "    chunks = np.array_split(list(X['id'].unique()), num_cuts)\n",
    "    rmse = []\n",
    "    \n",
    "    for i, ids in enumerate(chunks):\n",
    "        fit_index = list(X[~X['id'].isin(ids)].index)\n",
    "        score_index = list(X[X['id'].isin(ids)].index)\n",
    "        \n",
    "        y_hat = [prob[1] for prob in estimator.predict_proba(X.drop(columns=['id']))]\n",
    "        rmse.append(math.sqrt(mean_squared_error(y_hat, y)))\n",
    "    \n",
    "    return rmse, mean(rmse)\n",
    "\n",
    "class BaselineClassifier:\n",
    "    def __init__(self, winner_function):\n",
    "        self.winner_function = winner_function\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [1 if self.winner_function(X) else 0] + [0 if self.winner_function(X) else 1]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score([self.predict(feature)[1] for index, feature in X.iterrows()], y)\n",
    "    \n",
    "class BaselinePredictor:\n",
    "    def __init__(self, baseline_data):\n",
    "        self.baseline_data = baseline_data\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X, rating_difference=True):\n",
    "        probs = []\n",
    "        for index, feature in X.iterrows():\n",
    "            prob = self.baseline_data[self.baseline_data['rating_difference'] == round_to(feature['white_rating']-feature['black_rating'], 10)]['winning_percentage'].mean()\n",
    "            probs.append([prob, 1-prob])\n",
    "        return probs\n",
    "\n",
    "def model_eval(model):\n",
    "    print(f\"Accuracy Scores\")\n",
    "    print(f\"Random Forest Accuracy: {cross_val_accuracy(model, X, y)}\")\n",
    "    print(f\"Baseline Rating: {cross_val_accuracy(rating_baseline, X, y)}\")\n",
    "    print(f\"Baseline Eval: {cross_val_accuracy(eval_baseline, X, y)}\")\n",
    "\n",
    "    print(f\"\\nLoss Scores\")\n",
    "    print(f\"Random Forest RMSE: {cross_val_rmse(model, X, y)}\")\n",
    "    print(f\"Baseline RMSE: {cross_val_rmse(prob_baseline, X, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_baseline = BaselineClassifier(lambda row: row['rating_difference'] >= 0)\n",
    "eval_baseline = BaselineClassifier(lambda row: row['eval'] >= 0)\n",
    "prob_baseline = BaselinePredictor(baseline_data)\n",
    "rf = RandomForestClassifier(n_estimators=500, max_features=\"log2\", min_samples_leaf=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDummifier(cols=['perf'])\n",
    "ri = RatingImputer(cols=[column for column in raw_data.columns if 'rating' in column], strategy='0')\n",
    "rd = RatingDifferences(white_cols = [column for column in raw_data.columns if ('rating' in column) and ('white' in column)],\n",
    "                       black_cols = [column for column in raw_data.columns if ('rating' in column) and ('black' in column)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"dummify\", cd), ('imputer', ri), ('differences', rd)])\n",
    "\n",
    "data = pipeline.fit_transform(raw_data)\n",
    "\n",
    "X, y = create_labels_data(data, 'winner')\n",
    "\n",
    "model_eval(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDummifier(cols=['perf'])\n",
    "ri = RatingImputer(cols=[column for column in raw_data.columns if 'rating' in column], strategy='row_mean')\n",
    "rd = RatingDifferences(white_cols = [column for column in raw_data.columns if ('rating' in column) and ('white' in column)],\n",
    "                       black_cols = [column for column in raw_data.columns if ('rating' in column) and ('black' in column)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"dummify\", cd), ('imputer', ri), ('differences', rd)])\n",
    "\n",
    "data = pipeline.fit_transform(raw_data)\n",
    "\n",
    "X, y = create_labels_data(data, 'winner')\n",
    "\n",
    "model_eval(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDummifier(cols=['perf'])\n",
    "ri = RatingImputer(cols=[column for column in raw_data.columns if 'rating' in column], strategy='row_mean')\n",
    "rating_baseline = BaselineClassifier(lambda row: row['white_Rating'] >= row['black_Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"dummify\", cd), ('imputer', ri)])\n",
    "\n",
    "data = pipeline.fit_transform(raw_data)\n",
    "\n",
    "X, y = create_labels_data(data, 'winner')\n",
    "\n",
    "model_eval(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDummifier(cols=['perf'])\n",
    "ri = RatingImputer(cols=[column for column in raw_data.columns if 'rating' in column], strategy='row_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"dummify\", cd), ('imputer', ri)])\n",
    "\n",
    "data = pipeline.fit_transform(raw_data)\n",
    "\n",
    "X, y = create_labels_data(data, 'winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['id', 'perf_blitz', 'perf_bullet', 'perf_classical', 'perf_rapid',\n",
    "    'perf_ultrabullet', 'white_rating', 'white_ultrabullet_rating',\n",
    "    'white_bullet_rating', 'white_blitz_rating', 'white_rapid_rating',\n",
    "    'white_classical_rating', 'white_correspondence_rating', 'black_rating',\n",
    "    'black_ultrabullet_rating', 'black_bullet_rating', 'black_blitz_rating',\n",
    "    'black_rapid_rating', 'black_classical_rating', 'black_correspondence_rating',\n",
    "    'clock_initial', 'clock_increment', 'white_clock', 'black_clock', 'eval', 'is_mate']\n",
    "\n",
    "X = X[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='log2', min_samples_leaf=5,\n",
       "                       n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_features=\"log2\", min_samples_leaf=5, n_jobs=-1)\n",
    "rf.fit(X.drop(columns=['id']), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\rf.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(rf, r'..\\models\\rf.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
